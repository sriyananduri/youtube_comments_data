{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTtJriSYkW_2"
      },
      "source": [
        "Get video IDs from playlists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l-nGcatkVTV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "#YouTube API key\n",
        "api_key = ''\n",
        "\n",
        "playlist_ids = ['','']\n",
        "video_ids_file = 'video_ids.txt'\n",
        "last_video_id_file = 'last_video_id.txt'\n",
        "\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#function to get video IDs from playlists\n",
        "def get_video_ids_from_playlists(youtube, playlist_ids):\n",
        "    video_ids = []\n",
        "    from_date = ''\n",
        "    to_date = ''\n",
        "    for playlist_id in playlist_ids:\n",
        "        next_page_token = None\n",
        "        while True:\n",
        "            playlist_response = youtube.playlistItems().list(\n",
        "                part='snippet',\n",
        "                playlistId=playlist_id,\n",
        "                pageToken=next_page_token,\n",
        "                maxResults=50\n",
        "            ).execute()\n",
        "            for item in playlist_response['items']:\n",
        "                video_id = item['snippet']['resourceId']['videoId']\n",
        "                video_response = youtube.videos().list(\n",
        "                    part='snippet',\n",
        "                    id=video_id\n",
        "                ).execute()\n",
        "                video_published_at = video_response['items'][0]['snippet']['publishedAt']\n",
        "                if from_date <= video_published_at <= to_date:\n",
        "                    video_ids.append(video_id)\n",
        "            next_page_token = playlist_response.get('nextPageToken')\n",
        "            if not next_page_token:\n",
        "                break\n",
        "    return video_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#write the video IDs into the file video_ids.txt\n",
        "video_ids = get_video_ids_from_playlists(youtube, playlist_ids)\n",
        "with open(video_ids_file, 'w') as f:\n",
        "    for video_id in video_ids:\n",
        "        f.write(video_id + '\\n')\n",
        "\n",
        "#set the first video ID as the last video ID in the last_video_id.txt file\n",
        "with open(last_video_id_file, 'w') as f:\n",
        "    f.write(video_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj4bpe9WkYzt"
      },
      "source": [
        "Get 15k comments from each video ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4_Or1ELpJXZ"
      },
      "outputs": [],
      "source": [
        "comments_file = 'comments.csv'\n",
        "\n",
        "#get 15k comments from each video ID\n",
        "def get_comments_from_video(youtube, video_id):\n",
        "    comments = []\n",
        "    next_page_token = None\n",
        "    comment_count = 0\n",
        "    from_date = ''\n",
        "    to_date = ''\n",
        "    while True:\n",
        "        comment_response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            pageToken=next_page_token,\n",
        "            textFormat='plainText',\n",
        "            maxResults=100\n",
        "        ).execute()\n",
        "        for item in comment_response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']\n",
        "            comment_published_at = comment['publishedAt']\n",
        "            if from_date <= comment_published_at <= to_date:\n",
        "                comments.append({\n",
        "                    'Timestamp': comment['publishedAt'],\n",
        "                    'Username': comment['authorDisplayName'],\n",
        "                    'VideoID': video_id,\n",
        "                    'Comment': comment['textDisplay'],\n",
        "                    'Date': comment['publishedAt'],\n",
        "                    'LikeCount': comment['likeCount']\n",
        "                })\n",
        "            comment_count += 1\n",
        "            if comment_count >= 15000:\n",
        "                break\n",
        "        next_page_token = comment_response.get('nextPageToken')\n",
        "        if not next_page_token or comment_count >= 15000:\n",
        "            break\n",
        "    return comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#read video IDs from file\n",
        "with open(video_ids_file, 'r') as f:\n",
        "    video_ids = [line.strip() for line in f.readlines()]\n",
        "\n",
        "#read last video ID from file\n",
        "with open(last_video_id_file, 'r') as f:\n",
        "    last_video_id = f.read().strip()\n",
        "\n",
        "#find the index of the last video ID in the list of video IDs\n",
        "last_video_index = video_ids.index(last_video_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#loop through video IDs and get comments\n",
        "for video_id in video_ids[last_video_index:]:\n",
        "    comments = get_comments_from_video(youtube, video_id)\n",
        "    comments_df = pd.DataFrame(comments)\n",
        "    if os.path.exists(comments_file):\n",
        "        comments_df.to_csv(comments_file, mode='a', header=False, index=False)\n",
        "    else:\n",
        "        comments_df.to_csv(comments_file, index=False)\n",
        "\n",
        "    #update last video ID\n",
        "    with open(last_video_id_file, 'w') as f:\n",
        "        f.write(video_id)\n",
        "\n",
        "    #check if we've reached the end of the list of video IDs\n",
        "    if video_id == video_ids[-1]:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0iLv2x9aOhX"
      },
      "source": [
        "Get video statistics (likes, comment count, view count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E8UPX9yaQKU"
      },
      "outputs": [],
      "source": [
        "#function to get video statistics\n",
        "def get_video_stats(youtube, video_id):\n",
        "    video_response = youtube.videos().list(\n",
        "        part='statistics',\n",
        "        id=video_id\n",
        "    ).execute()\n",
        "    stats = video_response['items'][0]['statistics']\n",
        "    return {\n",
        "        'comments': stats['commentCount'],\n",
        "        'views': stats['viewCount'],\n",
        "        'likes': stats['likeCount']\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#read video IDs from file\n",
        "with open(video_ids_file, 'r') as f:\n",
        "    video_ids = [line.strip() for line in f.readlines()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get video statistics for each video ID\n",
        "stats = []\n",
        "for video_id in video_ids:\n",
        "    try:\n",
        "        video_stats = get_video_stats(youtube, video_id)\n",
        "        stats.append({\n",
        "            'video_id': video_id,\n",
        "            'comments': video_stats['comments'],\n",
        "            'views': video_stats['views'],\n",
        "            'likes': video_stats['likes']\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting stats for video {video_id}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#save statistics to CSV file\n",
        "df = pd.DataFrame(stats)\n",
        "df.to_csv('video_stats.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
